{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 3 - Convex Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "\n",
    "* To implement the constrained linear logistic regression.\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab3.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab3.ipynb\" file\n",
    "* Complete exercises in \"lab3.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "**Authors**\n",
    "* Kloe and Lydia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Logistic Regression\n",
    "* Let $\\{(\\mathbf{x}_1,y_1),\\cdots,(\\mathbf{x}_n,y_n)\\}$ be $n$ training examples. Denote $\\mathbf{x}_i\\in \\mathbb{R}^d$ as $i$-th feature and $y_i$ as its label. Our target is to learn a function $f(\\mathbf{x})$ from the training examples such that $f$ can predict the label $y$ of any feature $\\mathbf{x}$.\n",
    "\n",
    "* In linear logistic regression, we let $f(x)=\\mathbf{w}^{\\top}\\mathbf{x}$ where $\\mathbf{w}$ is the parameter of the function. \n",
    "\n",
    "* In order to learn a good $f$, according to tutorial 2, we usually solve the following problem:\n",
    "\n",
    "* $\\begin{equation} \\min_{\\mathbf{w}\\in\\mathbb{R}^d} NLL(\\mathbf{w})=\\sum_{i=1}^n \\log(1+\\exp(-y_i f(x_i,\\mathbf{w}))). \\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Constraints on the Parameters\n",
    "* When we optimize above linear logistic regression problem, sometimes, some entries of the parameter $\\mathbf{w}$ may become very large such that this model can memorize the training set very well. But the classifier may not generalize well on the test set. In this condition, we need to add some constraints on the parameters. \n",
    "* For example, we can assume that the parameters reside on a small sphere with radius $\\sqrt{\\gamma}$, i.e,\n",
    "* $\\begin{equation} \\begin{split} \\min_{\\mathbf{w}\\in\\mathbb{R}^d} \\quad &NLL(\\mathbf{w})=\\sum_{i=1}^n \\log(1+\\exp(-y_i f(x_i,\\mathbf{w}))), \\\\ \\textrm{subject to} \\quad & \\sum_{i=1}^d w_i^2 = \\gamma. \\end{split} \\end{equation}$\n",
    "* This is a constrained convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Lagrangian Function and ADMM\n",
    "* The Lagrangian function is:\n",
    "* $\\begin{equation}  \\min_{\\mathbf{w}\\in\\mathbb{R}^d} \\max_{\\lambda} \\quad \\mathcal{L}(\\mathbf{w},\\lambda)=\\sum_{i=1}^n \\log(1+\\exp(-y_i f(x_i,\\mathbf{w}))) + \\lambda(\\sum_{i=1}^d w_i^2 - \\gamma). \\end{equation}$\n",
    "* $\\lambda$ is known as the Lagrangian multiplier. \n",
    "* Here, we solve the primal-dual problem using alternating direction method of multipliers (ADMM). Specifically, in each iteration, we update $\\mathbf{w}$ using the gradient descent method and update $\\lambda$ using the gradient ascent method, which is shown as following:\n",
    "* $\\begin{equation} \\begin{split} \\mathbf{w}^{(k+1)} &= \\mathbf{w}^{(k)}-\\alpha_1 \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}|_{\\mathbf{w}^{(k)}}; \\\\  \\lambda^{(k+1)} &= \\lambda^{(k)} + \\alpha_2 \\frac{\\partial \\mathcal{L}}{\\partial \\lambda}|_{\\lambda^{(k)}}. \\end{split} \\end{equation}$\n",
    "* Here, $k$ refers to the number of iterations, $\\alpha_1$ and $\\alpha_2$ are step sizes for updating $\\mathbf{w}$ and $\\lambda$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implementation of ADMM Algorithm\n",
    "* Now we start to implement our method.\n",
    "* Here, we also take the binary classification as an example. We use the Iris example in tutorials 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Load Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Split the Dataset\n",
    "* Note that, to avoid overfitting or to conduct model selection, we usually split the dataset to training, validation, and test data. \n",
    "* Here, for simplicity, we directly split the data to training and test data. Must remember that for academic research, validation set is very important and necessary.\n",
    "* More details about training, test and validation sets can be found at https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Compute the Gradient of Lagrangian Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3.1**\n",
    "* Finish the code for computing graident of $\\mathcal{L}$ with respect to $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Optimization of the Overall Problem\n",
    "\n",
    "** Exercise 1.3.2**\n",
    "* Finish the overall algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 Prediction and Test Accuracy\n",
    "* Test the performance of our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
