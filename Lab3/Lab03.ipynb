{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 3 - Convex Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "\n",
    "* To implement the constrained linear logistic regression.\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "   \n",
    "* If you are using Jupyter intalled on your computer, Go to File->Open. Drag and drop \"lab3.ipynb\" file to the home interface and click upload. \n",
    "* If you are using Google Colaboratory, Click File->Upload notebook, and and upload \"lab3.ipynb\" file\n",
    "* Complete exercises in \"Lab03.ipynb\".\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top.\n",
    "* Complete all exercises marked with **TODO**.\n",
    "* Save your file when you are done with the exercises, so you can show your tutor next week.\n",
    "\n",
    "\n",
    "**Authors**\n",
    "* Kloe and Lydia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Logistic Regression\n",
    "* Let $\\{(\\mathbf{x}_1,y_1),\\cdots,(\\mathbf{x}_n,y_n)\\}$ be $n$ training examples. Denote $\\mathbf{x}_i\\in \\mathbb{R}^d$ as $i$-th feature and $y_i$ as its label. Our target is to learn a function $f(\\mathbf{x})$ from the training examples such that $f$ can predict the label $y$ of any feature $\\mathbf{x}$.\n",
    "\n",
    "* In linear logistic regression, we let $f(x)=\\mathbf{w}^{\\top}\\mathbf{x}$ where $\\mathbf{w}$ is the parameter of the function. \n",
    "\n",
    "* In order to learn a good $f$, according to tutorial 2, we usually solve the following problem:\n",
    "\n",
    "* $\\begin{equation} \\min_{\\mathbf{w}\\in\\mathbb{R}^d} NLL(\\mathbf{w})=\\sum_{i=1}^n \\log(1+\\exp(-y_i f(x_i,\\mathbf{w}))). \\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Constraints on the Parameters\n",
    "* When we optimize above linear logistic regression problem, sometimes, some entries of the parameter $\\mathbf{w}$ may become very large such that this model can memorize the training set very well. But the classifier may not generalize well on the test set. In this condition, we need to add some constraints on the parameters. \n",
    "* For example, we can assume that the parameters reside on a small sphere with radius $\\sqrt{\\gamma}$, i.e,\n",
    "* $\\begin{equation} \\begin{split} \\min_{\\mathbf{w}\\in\\mathbb{R}^d} \\quad &NLL(\\mathbf{w})=\\sum_{i=1}^n \\log(1+\\exp(-y_i f(x_i,\\mathbf{w}))), \\\\ \\textrm{subject to} \\quad & \\sum_{i=1}^d w_i^2 = \\gamma. \\end{split} \\end{equation}$\n",
    "* This is a constrained convex optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Lagrangian Function and ADMM\n",
    "* The Lagrangian function is:\n",
    "* $\\begin{equation}  \\min_{\\mathbf{w}\\in\\mathbb{R}^d} \\max_{\\lambda} \\quad \\mathcal{L}(\\mathbf{w},\\lambda)=\\sum_{i=1}^n \\log(1+\\exp(-y_i f(x_i,\\mathbf{w}))) + \\lambda(\\sum_{i=1}^d w_i^2 - \\gamma). \\end{equation}$\n",
    "* $\\lambda$ is known as the Lagrangian multiplier. \n",
    "* Here, we solve the primal-dual problem using alternating direction method of multipliers (ADMM). Specifically, in each iteration, we update $\\mathbf{w}$ using the gradient descent method and update $\\lambda$ using the gradient ascent method, which is shown as following:\n",
    "* $\\begin{equation} \\begin{split} \\mathbf{w}^{(k+1)} &= \\mathbf{w}^{(k)}-\\alpha_1 \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}|_{\\mathbf{w}^{(k)}}; \\\\  \\lambda^{(k+1)} &= \\lambda^{(k)} + \\alpha_2 \\frac{\\partial \\mathcal{L}}{\\partial \\lambda}|_{\\lambda^{(k)}}. \\end{split} \\end{equation}$\n",
    "* Here, $k$ refers to the number of iterations, $\\alpha_1$ and $\\alpha_2$ are step sizes for updating $\\mathbf{w}$ and $\\lambda$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implementation of ADMM Algorithm\n",
    "* Now we start to implement our method.\n",
    "* Here, we also take the binary classification as an example. We use the Iris example in tutorials 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Load Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "data = CSV.read(\"iris.csv\")\n",
    "\n",
    "features = Array{Float64}[]\n",
    "target = []\n",
    "for i in 1:size(data,1)\n",
    "    if data.species[i] == \"setosa\"\n",
    "        push!(features,[data.petal_length[i],data.petal_width[i],data.sepal_length[i],data.sepal_width[i]])\n",
    "        push!(target,1)\n",
    "    elseif data.species[i] == \"versicolor\"\n",
    "        push!(features,[data.petal_length[i],data.petal_width[i],data.sepal_length[i],data.sepal_width[i]])\n",
    "        push!(target,-1)\n",
    "    end\n",
    "end\n",
    "\n",
    "n = length(target)\n",
    "dim = 4\n",
    "X = zeros(n,dim)\n",
    "Y = zeros(n)\n",
    "for i in 1:n\n",
    "    for j in 1:dim\n",
    "        X[i,j] = features[i][j]\n",
    "    end\n",
    "    Y[i] = target[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Split the Dataset\n",
    "* Note that, to avoid overfitting or to conduct model selection, we usually split the dataset to training, validation, and test data. \n",
    "* Here, for simplicity, we directly split the data to training and test data. Must remember that for academic research, validation set is very important and necessary.\n",
    "* More details about training, test and validation sets can be found at https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 20)"
     ]
    }
   ],
   "source": [
    "using Random \n",
    "# shuffle index\n",
    "rng = Random.seed!()\n",
    "arr = shuffle(rng,1:n)\n",
    "\n",
    "# split the data to training (80%) and test data (20%)\n",
    "n_train = floor(Int,0.8*n)\n",
    "n_test = n - n_train\n",
    "\n",
    "# training data\n",
    "x_train = X[arr[1:n_train],:]\n",
    "x_train = vcat(x_train',ones(1,n_train)) # add offset to data points because we consider the bias in the model\n",
    "y_train = Y[arr[1:n_train],:]\n",
    "\n",
    "# test data\n",
    "x_test = X[arr[n_train+1:end],:]\n",
    "x_test = vcat(x_test',ones(1,n_test))\n",
    "y_test = y[arr[n_train+1:end]]\n",
    "print(size(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Compute the Gradient of Lagrangian Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3.1**\n",
    "* Finish the code for computing graident of $\\mathcal{L}$ with respect to $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_w (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function grad_lambda(w,gamma)\n",
    "    \"\"\"Compute the gradient of Lagrangian function with respect to lambda.\n",
    "    Input:\n",
    "        w: array, shape = [d+1]. The parameter of the classifier.\n",
    "        gamma: real value. The square of the radius of the sphere.\n",
    "    Output:\n",
    "        grad: real value. The gradient of L with respect to lambda.\n",
    "    \"\"\"\n",
    "    return norm(w)^2 - gamma \n",
    "end\n",
    "\n",
    "function grad_w(X,Y,w,labda)\n",
    "    \"\"\"Compute the gradient of Lagrangian function with respect to w.\n",
    "    Input:\n",
    "        X: array, shape = [d+1,n], features\n",
    "        Y: array, shape = [n], labels\n",
    "        w: array, shape = [d+1], current value of w.\n",
    "        labda: real value, the dual variable lambda.\n",
    "    Output:\n",
    "        grad: array, shape = [d+1]\n",
    "            The gradient of L with respect to w.\n",
    "    \"\"\"\n",
    "    d,n = size(X)\n",
    "    grad = zeros(d)\n",
    "    \n",
    "    temp = zeros(n)\n",
    "    for i in 1:n\n",
    "        u = 1.0/(1.0+exp(-dot(X[:,i],w)))\n",
    "        if Y[i] == 1\n",
    "            temp[i] = u-1\n",
    "        else\n",
    "            temp[i] = u\n",
    "        end\n",
    "    end\n",
    "    grad = X*temp + 2*labda*w\n",
    "    \n",
    "    return grad\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Optimization of the Overall Problem\n",
    "\n",
    "** Exercise 1.3.2**\n",
    "* Finish the overall algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.277959, -0.115293, 0.0346117, 0.171608, 0.0279012]\n",
      "59.82324645682874\n",
      "0.12197965660900956"
     ]
    }
   ],
   "source": [
    "function constrainedLR_admm(X,Y,gamma=0.1,alpha_1 = 1e-3,alpha_2=1e-0, maxiter=500, tol=1e-8)\n",
    "    \"\"\"Optimizing the constrained logistic regression using ADMM\n",
    "    Input: \n",
    "       X: array, shape = [d+1, n], Features.\n",
    "       Y: array, shape = [n], Labels.\n",
    "       gamma: real value. The square of the radius of the sphere.\n",
    "       alpha_1 and alpha_2: real values. The step size or learning rate for both w and lambda.\n",
    "       maxiter: real value. The maximum number of iterations.\n",
    "       tol: real value. Stop criteria.\n",
    "    Output:\n",
    "       w: array, shape = [d+1]. The estimated w.\n",
    "       lamb: real value. The estimated lambda.\n",
    "    \"\"\"\n",
    "    # Initialise w and lambda \n",
    "    d,n = size(X)\n",
    "    w = 0.01*rand(d)\n",
    "    labda = 0\n",
    "    \n",
    "    # we use the norm of gradient of L w.r.t. w as a stop criteria.\n",
    "    # If it is smaller than tol, then update stops.\n",
    "    diff_ = Inf\n",
    "    iters = 0\n",
    "    \n",
    "    while (iters < maxiter) & (diff_ > tol)\n",
    "        # first: update w\n",
    "        gradw = grad_w(X,Y,w,labda)\n",
    "        w -= alpha_1 * gradw\n",
    "        diff_ = norm(gradw)\n",
    "        \n",
    "        # second: update labda\n",
    "        gradlambda = grad_lambda(w,gamma)\n",
    "        labda += alpha_2 * gradlambda\n",
    "\n",
    "        iters += 1\n",
    "    end\n",
    "    \n",
    "    return w, labda\n",
    "end\n",
    "\n",
    "w,labda = constrainedLR_admm(x_train,y_train)\n",
    "println(w)\n",
    "println(labda)\n",
    "print(norm(w)^2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 Prediction and Test Accuracy\n",
    "* Test the performance of our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0"
     ]
    }
   ],
   "source": [
    "function predict(w,X,Y)\n",
    "    d,n = size(X)\n",
    "    acc = 0\n",
    "    for i in 1:n\n",
    "        u = 1.0/(1.0+exp(-dot(X[:,i],w)))\n",
    "        if (u>0.5) & (Y[i] == 1)\n",
    "            acc+=1\n",
    "        elseif (u<=0.5) & (Y[i] == -1)\n",
    "            acc+=1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    acc /= size(Y)[1]\n",
    "    \n",
    "    return acc\n",
    "end\n",
    "\n",
    "acc = predict(w,x_test,y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
